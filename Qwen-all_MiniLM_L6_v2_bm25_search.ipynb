{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag1NfxGfGGkB"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Installation\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q sentence-transformers[faiss-gpu] rank-bm25 datasets pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isrPN9MFGZl5"
      },
      "outputs": [],
      "source": [
        "# Cell 2: GPU Verification\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"GPU not detected! Go to Runtime → Change runtime type → GPU\"\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHm0XSkZGcOB"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65NxbM65GfQx"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Load Dataset\n",
        "train_ds = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"train\")\n",
        "print(f\"Train examples: {len(train_ds):,}\")\n",
        "\n",
        "example = train_ds[0]\n",
        "print(f\"\\nQuestion: {example['question']}\")\n",
        "print(f\"Answer: {example['answer']}\")\n",
        "print(f\"Supporting Facts - Titles: {example['supporting_facts']['title']}\")\n",
        "print(f\"Supporting Facts - Sent IDs: {example['supporting_facts']['sent_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkyJWnKoGj-b"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Flatten Dataset to Sentence-Level\n",
        "def dataset_to_dataframe(ds):\n",
        "    rows = []\n",
        "    for ex in tqdm(ds, desc=\"Flattening to sentences\"):\n",
        "        q = ex[\"question\"]\n",
        "        qid = ex[\"id\"]\n",
        "        answer = ex[\"answer\"]\n",
        "\n",
        "        supporting_facts = set(\n",
        "            zip(ex[\"supporting_facts\"][\"title\"], ex[\"supporting_facts\"][\"sent_id\"])\n",
        "        )\n",
        "\n",
        "        for title, sentences in zip(ex[\"context\"][\"title\"], ex[\"context\"][\"sentences\"]):\n",
        "            for sent_id, sentence in enumerate(sentences):\n",
        "                is_relevant = 1 if (title, sent_id) in supporting_facts else 0\n",
        "\n",
        "                rows.append({\n",
        "                    \"query_id\": qid,\n",
        "                    \"query\": q,\n",
        "                    \"passage_text\": sentence,\n",
        "                    \"passage_title\": title,\n",
        "                    \"sent_id\": sent_id,\n",
        "                    \"label\": is_relevant,\n",
        "                    \"answer\": answer\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "train_df = dataset_to_dataframe(train_ds)\n",
        "print(f\"\\nTotal sentence rows: {len(train_df):,}\")\n",
        "print(f\"Positive sentences: {train_df['label'].sum():,}\")\n",
        "print(f\"Negative sentences: {(train_df['label'] == 0).sum():,}\")\n",
        "print(f\"Avg positives per query: {train_df['label'].sum() / train_df['query_id'].nunique():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h7yEP_EGq8N"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Build BM25 Index\n",
        "unique_passages = train_df[\"passage_text\"].drop_duplicates().tolist()\n",
        "print(f\"\\nUnique passages: {len(unique_passages):,}\")\n",
        "print(\"Tokenizing for BM25...\")\n",
        "tokenized_corpus = [doc.lower().split() for doc in tqdm(unique_passages)]\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-gsk-aTGtOL"
      },
      "outputs": [],
      "source": [
        "# Cell 7: BM25 Search Function\n",
        "def search_bm25(query: str, top_k: int = 10):\n",
        "    tokens = query.lower().split()\n",
        "    scores = bm25.get_scores(tokens)\n",
        "    top_idx = np.argpartition(-scores, range(top_k))[:top_k]\n",
        "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
        "    return [\n",
        "        {\"rank\": i+1, \"score\": float(scores[idx]), \"passage\": unique_passages[idx]}\n",
        "        for i, idx in enumerate(top_idx)\n",
        "    ]\n",
        "\n",
        "print(\"\\n=== BM25 Test ===\")\n",
        "for r in search_bm25(\"Which magazine was started first Arthur's Magazine or First for Women?\", 5):\n",
        "    print(f\"[{r['rank']}] {r['score']:.2f} | {r['passage'][:150]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nZ5PNfFGvSE"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Create Training Examples\n",
        "print(\"\\nBuilding training examples...\")\n",
        "train_examples = []\n",
        "\n",
        "for ex in tqdm(train_ds, desc=\"Creating training pairs\"):\n",
        "    query = ex[\"question\"]\n",
        "\n",
        "    supporting_facts = set(\n",
        "        zip(ex[\"supporting_facts\"][\"title\"], ex[\"supporting_facts\"][\"sent_id\"])\n",
        "    )\n",
        "\n",
        "    if not supporting_facts:\n",
        "        continue\n",
        "\n",
        "    for title, sentences in zip(ex[\"context\"][\"title\"], ex[\"context\"][\"sentences\"]):\n",
        "        for sent_id, sentence in enumerate(sentences):\n",
        "            if (title, sent_id) in supporting_facts:\n",
        "                train_examples.append(InputExample(texts=[query, sentence]))\n",
        "\n",
        "print(f\"Training pairs: {len(train_examples):,}\")\n",
        "print(f\"Average positives per query: {len(train_examples) / len(train_ds):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apOSXAUHGxes"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Create Validation Set\n",
        "print(\"\\nBuilding validation set...\")\n",
        "val_queries = []\n",
        "val_positives = []\n",
        "val_scores = []\n",
        "seen_queries = set()\n",
        "\n",
        "for example in tqdm(train_ds, desc=\"Validation set\"):\n",
        "    query = example[\"question\"]\n",
        "\n",
        "    if query in seen_queries:\n",
        "        continue\n",
        "    seen_queries.add(query)\n",
        "\n",
        "    supporting_facts = set(\n",
        "        zip(example[\"supporting_facts\"][\"title\"], example[\"supporting_facts\"][\"sent_id\"])\n",
        "    )\n",
        "\n",
        "    if not supporting_facts:\n",
        "        continue\n",
        "\n",
        "    positives = []\n",
        "    negatives = []\n",
        "\n",
        "    for title, sentences in zip(example[\"context\"][\"title\"], example[\"context\"][\"sentences\"]):\n",
        "        for sent_id, sentence in enumerate(sentences):\n",
        "            if (title, sent_id) in supporting_facts:\n",
        "                positives.append(sentence)\n",
        "            else:\n",
        "                negatives.append(sentence)\n",
        "\n",
        "    if positives and len(negatives) >= 10:\n",
        "        for pos_sentence in positives:\n",
        "            val_queries.append(query)\n",
        "            val_positives.append(pos_sentence)\n",
        "            val_scores.append(1.0)\n",
        "\n",
        "        selected_negatives = random.sample(negatives, 10)\n",
        "        for neg_sentence in selected_negatives:\n",
        "            val_queries.append(query)\n",
        "            val_positives.append(neg_sentence)\n",
        "            val_scores.append(0.0)\n",
        "\n",
        "    if len(seen_queries) >= 5000:\n",
        "        break\n",
        "\n",
        "print(f\"Final validation pairs: {len(val_queries)}\")\n",
        "print(f\"Unique queries: {len(seen_queries)}\")\n",
        "print(f\"Avg positives per query: {val_scores.count(1.0) / len(seen_queries):.2f}\")\n",
        "print(f\"Avg negatives per query: {val_scores.count(0.0) / len(seen_queries):.2f}\")\n",
        "\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_queries,\n",
        "    sentences2=val_positives,\n",
        "    scores=val_scores,\n",
        "    batch_size=128,\n",
        "    name=\"hotpotqa-val\",\n",
        "    show_progress_bar=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFmMvL5rG1vW"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Training Configuration\n",
        "print(\"\\nLoading base model on GPU...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = model.to('cuda')\n",
        "\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=model, scale=20.0)\n",
        "dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)\n",
        "\n",
        "best_score = -1.0\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "max_epochs = 20\n",
        "learning_rates = [2e-5, 2e-5, 2e-5, 2e-5, 2e-5, 2e-5, 2e-5,\n",
        "                  1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5,\n",
        "                  2e-6, 2e-6, 2e-6, 2e-6, 2e-6, 2e-6, 2e-6]\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "print(f\"\\nTotal epochs: {max_epochs}\")\n",
        "print(f\"Batch size: 64\")\n",
        "print(f\"Patience: {patience}\")\n",
        "print(f\"LR schedule: 2e-5 (epochs 1-7) → 1e-5 (epochs 8-13) → 2e-6 (epochs 14-20)\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlMeF55zG5RQ"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Training Loop\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    current_lr = learning_rates[epoch - 1]\n",
        "    print(f\"\\n=== Epoch {epoch}/{max_epochs} (LR: {current_lr}) ===\")\n",
        "\n",
        "    model.fit(\n",
        "        train_objectives=[(dataloader, train_loss)],\n",
        "        epochs=1,\n",
        "        warmup_steps=int(0.1 * len(dataloader)),\n",
        "        optimizer_params={'lr': current_lr},\n",
        "        show_progress_bar=True,\n",
        "    )\n",
        "\n",
        "    print(\"Evaluating on validation set...\")\n",
        "    eval_result = evaluator(model)\n",
        "\n",
        "    if isinstance(eval_result, dict):\n",
        "        score = (eval_result.get(\"cosine_pearson\") or\n",
        "                 eval_result.get(\"pearson_cosine\") or\n",
        "                 eval_result.get(\"cosine_spearman\") or\n",
        "                 eval_result.get(\"spearman_cosine\") or\n",
        "                 list(eval_result.values())[0])\n",
        "        spearman = (eval_result.get(\"cosine_spearman\") or\n",
        "                    eval_result.get(\"spearman_cosine\") or\n",
        "                    score)\n",
        "        print(f\"Validation → Main metric: {score:.5f} | Spearman: {spearman:.5f}\")\n",
        "    else:\n",
        "        score = eval_result\n",
        "        print(f\"Validation score: {score:.5f}\")\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        checkpoint_path = f\"models/minilm-hotpotqa-epoch{epoch}-score{score:.4f}\"\n",
        "        model.save(checkpoint_path)\n",
        "        print(f\"✓ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    if score > best_score + 0.001:\n",
        "        best_score = score\n",
        "        patience_counter = 0\n",
        "        best_model_path = f\"models/minilm-hotpotqa-best-score{best_score:.4f}\"\n",
        "        model.save(best_model_path)\n",
        "        print(f\"✓ New best model saved: {best_model_path}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement ({patience_counter}/{patience})\")\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQFDQfSNHDJ8"
      },
      "outputs": [],
      "source": [
        "# Cell 12: Load Best Model and Save Final\n",
        "print(\"\\n=== Loading best model for final save ===\")\n",
        "best_models = [d for d in os.listdir(\"models\") if d.startswith(\"minilm-hotpotqa-best\")]\n",
        "if best_models:\n",
        "    best_model_dir = sorted(best_models)[-1]\n",
        "    model = SentenceTransformer(f\"models/{best_model_dir}\")\n",
        "    model = model.to('cuda')\n",
        "    print(f\"Loaded: {best_model_dir}\")\n",
        "\n",
        "final_path = \"models/minilm-hotpotqa-finetuned-final\"\n",
        "model.save(final_path)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training finished!\")\n",
        "print(f\"Best validation score: {best_score:.5f}\")\n",
        "print(f\"Total epochs completed: {epoch}\")\n",
        "print(f\"Model saved to: {final_path}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Cell 13: Performance Test\n",
        "print(\"\\n=== Quick Performance Test ===\")\n",
        "test_queries = [\n",
        "    \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who invented the telephone?\"\n",
        "]\n",
        "\n",
        "for test_q in test_queries:\n",
        "    print(f\"\\nQuery: {test_q}\")\n",
        "    query_emb = model.encode(test_q, convert_to_tensor=True)\n",
        "\n",
        "    corpus_sample = random.sample(unique_passages, min(1000, len(unique_passages)))\n",
        "    corpus_embs = model.encode(corpus_sample, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "    scores = util.cos_sim(query_emb, corpus_embs)[0]\n",
        "    top_idx = torch.topk(scores, k=5).indices\n",
        "\n",
        "    for i, idx in enumerate(top_idx):\n",
        "        print(f\"  [{i+1}] {scores[idx]:.4f} | {corpus_sample[idx][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrNX97MVHFfU"
      },
      "outputs": [],
      "source": [
        "# Cell 14: Save to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p \"/content/drive/MyDrive/Colab Models\"\n",
        "!cp -r models \"/content/drive/MyDrive/Colab Models/hotpotqa-minilm-$(date +%Y%m%d-%H%M%S)\"\n",
        "print(\"\\n All models copied to your Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu42SRRrHHJp"
      },
      "outputs": [],
      "source": [
        "# Cell 15: Load Model\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Interview/minilm-hotpotqa-epoch15-score0.5296\"\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Interview/HotpotQA_Dataset_20251128\"\n",
        "\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "BATCH_SIZE = 256 if USE_GPU else 128\n",
        "USE_FP16 = USE_GPU\n",
        "NUM_WORKERS = min(4, mp.cpu_count())\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"ERROR: Model not found at {MODEL_PATH}\")\n",
        "    print(\"Available models in Interview folder:\")\n",
        "    interview_path = \"/content/drive/MyDrive/Interview\"\n",
        "    if os.path.exists(interview_path):\n",
        "        for item in os.listdir(interview_path):\n",
        "            print(f\"  - {item}\")\n",
        "    exit()\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
        "    exit()\n",
        "\n",
        "print(f\" Model path verified: {MODEL_PATH}\")\n",
        "print(f\" Dataset path verified: {DATASET_PATH}\")\n",
        "model = SentenceTransformer(MODEL_PATH)\n",
        "device = 'cuda' if USE_GPU else 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "if USE_GPU:\n",
        "    model.half()\n",
        "    print(f\" Model loaded on GPU with FP16: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(f\" Model loaded on CPU\")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh66jeMeHukQ"
      },
      "outputs": [],
      "source": [
        "# Cell 16: Load Dataset\n",
        "print(\"\\n[2/7] Loading validation dataset...\")\n",
        "try:\n",
        "    val_df = pd.read_parquet(f\"{DATASET_PATH}/validation_sentences.parquet\")\n",
        "    print(f\" Loaded from parquet: {len(val_df):,} sentences\")\n",
        "except FileNotFoundError:\n",
        "    try:\n",
        "        val_df = pd.read_csv(f\"{DATASET_PATH}/validation_sentences.csv\")\n",
        "        print(f\" Loaded from CSV: {len(val_df):,} sentences\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"ERROR: Could not find validation_sentences.parquet or .csv\")\n",
        "        exit()\n",
        "\n",
        "print(f\"  Supporting sentences: {val_df['is_supporting'].sum():,}\")\n",
        "print(f\"  Unique questions: {val_df['id'].nunique():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtv3pLRbH89h"
      },
      "outputs": [],
      "source": [
        "# Cell 17: Prepare and encode Corpus\n",
        "print(\"\\n[3/7] Preparing corpus with deduplication...\")\n",
        "corpus_df = val_df[['context_title', 'context_sentence', 'sent_id']].drop_duplicates(\n",
        "    subset=['context_sentence'],\n",
        "    keep='first'\n",
        ").reset_index(drop=True)\n",
        "\n",
        "corpus = corpus_df['context_sentence'].tolist()\n",
        "print(f\" Unique sentences in corpus: {len(corpus):,}\")\n",
        "print(f\"  Deduplication ratio: {len(corpus) / len(val_df) * 100:.1f}% of original\")\n",
        "\n",
        "print(\"\\n[4/7] Encoding corpus with trained model...\")\n",
        "print(f\"Settings: Batch size={BATCH_SIZE}, FP16={USE_FP16}, Device={device}\")\n",
        "\n",
        "def encode_corpus_optimized(model, corpus: List[str], batch_size: int, device: str) -> np.ndarray:\n",
        "    all_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(corpus), batch_size), desc=\"Encoding batches\"):\n",
        "            batch = corpus[i:i+batch_size]\n",
        "\n",
        "            embeddings = model.encode(\n",
        "                batch,\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False,\n",
        "                batch_size=batch_size,\n",
        "                normalize_embeddings=False\n",
        "            )\n",
        "\n",
        "            if device == 'cuda':\n",
        "                embeddings = embeddings.cpu()\n",
        "\n",
        "            all_embeddings.append(embeddings)\n",
        "\n",
        "            if USE_GPU and i % (batch_size * 10) == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    corpus_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    return corpus_embeddings.numpy().astype('float32')\n",
        "\n",
        "corpus_embeddings_np = encode_corpus_optimized(model, corpus, BATCH_SIZE, device)\n",
        "print(f\"✓ Corpus encoded: {corpus_embeddings_np.shape}\")\n",
        "print(f\"  Embedding dimension: {corpus_embeddings_np.shape[1]}\")\n",
        "print(f\"  Total size: {corpus_embeddings_np.nbytes / 1e6:.2f} MB\")\n",
        "\n",
        "if USE_GPU:\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6K9gXnFIHS-"
      },
      "outputs": [],
      "source": [
        "# Cell 18: Build FAISS Index\n",
        "print(\"\\n[5/7] Building optimized FAISS index...\")\n",
        "dimension = corpus_embeddings_np.shape[1]\n",
        "num_vectors = corpus_embeddings_np.shape[0]\n",
        "\n",
        "print(f\"  Dimension: {dimension}\")\n",
        "print(f\"  Vectors: {num_vectors:,}\")\n",
        "\n",
        "if num_vectors < 100000:\n",
        "    index_type = \"IndexFlatIP\"\n",
        "    print(f\"  Index type: {index_type} (Exact search)\")\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "elif num_vectors < 1000000:\n",
        "    index_type = \"IndexIVFFlat\"\n",
        "    nlist = min(4096, int(np.sqrt(num_vectors)))\n",
        "    print(f\"  Index type: {index_type} (nlist={nlist})\")\n",
        "    quantizer = faiss.IndexFlatIP(dimension)\n",
        "    index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "else:\n",
        "    index_type = \"IndexIVFFlat\"\n",
        "    nlist = min(16384, int(np.sqrt(num_vectors) * 2))\n",
        "    print(f\"  Index type: {index_type} (nlist={nlist})\")\n",
        "    quantizer = faiss.IndexFlatIP(dimension)\n",
        "    index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "print(\"  Normalizing embeddings for cosine similarity...\")\n",
        "faiss.normalize_L2(corpus_embeddings_np)\n",
        "\n",
        "if USE_GPU:\n",
        "    try:\n",
        "        res = faiss.StandardGpuResources()\n",
        "        res.setTempMemory(1024 * 1024 * 512)\n",
        "\n",
        "        gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "        print(\"  ✓ Using GPU for FAISS indexing\")\n",
        "\n",
        "        if 'IVF' in index_type:\n",
        "            print(\"  Training IVF index...\")\n",
        "            train_size = min(num_vectors, 100000)\n",
        "            train_indices = np.random.choice(num_vectors, train_size, replace=False)\n",
        "            gpu_index.train(corpus_embeddings_np[train_indices])\n",
        "\n",
        "        print(\"  Adding vectors to index...\")\n",
        "        gpu_index.add(corpus_embeddings_np)\n",
        "\n",
        "        index = faiss.index_gpu_to_cpu(gpu_index)\n",
        "        print(f\"  ✓ GPU indexing complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠ GPU indexing failed ({e}), falling back to CPU\")\n",
        "        if 'IVF' in index_type:\n",
        "            print(\"  Training IVF index on CPU...\")\n",
        "            train_size = min(num_vectors, 100000)\n",
        "            train_indices = np.random.choice(num_vectors, train_size, replace=False)\n",
        "            index.train(corpus_embeddings_np[train_indices])\n",
        "        index.add(corpus_embeddings_np)\n",
        "else:\n",
        "    print(\"  Using CPU for FAISS indexing\")\n",
        "    if 'IVF' in index_type:\n",
        "        print(\"  Training IVF index...\")\n",
        "        train_size = min(num_vectors, 100000)\n",
        "        train_indices = np.random.choice(num_vectors, train_size, replace=False)\n",
        "        index.train(corpus_embeddings_np[train_indices])\n",
        "    index.add(corpus_embeddings_np)\n",
        "\n",
        "print(f\"✓ FAISS index built with {index.ntotal:,} vectors\")\n",
        "\n",
        "if 'IVF' in index_type:\n",
        "    index.nprobe = min(64, index.nlist // 4)\n",
        "    print(f\"  Set nprobe={index.nprobe} for search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfRxPufNIR6J"
      },
      "outputs": [],
      "source": [
        "# Cell 19: Save Index and Corpus\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "\n",
        "index_path = f\"{DATASET_PATH}/faiss_index.bin\"\n",
        "faiss.write_index(index, index_path)\n",
        "index_size_mb = os.path.getsize(index_path) / 1e6\n",
        "print(f\"✓ FAISS index saved: {index_path}\")\n",
        "print(f\"  File size: {index_size_mb:.2f} MB\")\n",
        "\n",
        "corpus_path = f\"{DATASET_PATH}/corpus.pkl\"\n",
        "corpus_data = {\n",
        "    'corpus': corpus,\n",
        "    'corpus_df': corpus_df,\n",
        "    'model_name': MODEL_PATH,\n",
        "    'embedding_dim': dimension,\n",
        "    'num_vectors': num_vectors,\n",
        "    'index_type': index_type\n",
        "}\n",
        "\n",
        "with open(corpus_path, 'wb') as f:\n",
        "    pickle.dump(corpus_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "corpus_size_mb = os.path.getsize(corpus_path) / 1e6\n",
        "print(f\"✓ Corpus metadata saved: {corpus_path}\")\n",
        "print(f\"  File size: {corpus_size_mb:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ8PdPD4Ia0q"
      },
      "outputs": [],
      "source": [
        "# Cell 20: Test Index\n",
        "print(\"\\n[7/7] Running comprehensive test...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_queries = [\n",
        "    \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who won the Nobel Prize in Physics?\"\n",
        "]\n",
        "\n",
        "for test_query in test_queries:\n",
        "    print(f\"\\nTest query: {test_query}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_emb = model.encode(test_query, convert_to_tensor=True, show_progress_bar=False)\n",
        "        if USE_GPU:\n",
        "            query_emb = query_emb.cpu()\n",
        "        query_emb_np = query_emb.numpy().astype('float32').reshape(1, -1)\n",
        "\n",
        "    faiss.normalize_L2(query_emb_np)\n",
        "\n",
        "    k = 5\n",
        "    scores, indices = index.search(query_emb_np, k)\n",
        "\n",
        "    print(f\"Top {k} retrieved sentences:\")\n",
        "    for rank, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):\n",
        "        print(f\"  [{rank}] Score: {score:.4f}\")\n",
        "        print(f\"      {corpus[idx][:120]}...\")\n",
        "  print(\"\\n\" + \"=\"*80)\n",
        "print(\" INDEX BUILD COMPLETE!\")\n",
        "print(f\"\\nOptimization Summary:\")\n",
        "print(f\"  Device: {device.upper()}\")\n",
        "print(f\"  FP16 enabled: {USE_FP16}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Index type: {index_type}\")\n",
        "if 'IVF' in index_type:\n",
        "    print(f\"  Search nprobe: {index.nprobe}\")\n",
        "\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  1. {index_path} ({index_size_mb:.2f} MB)\")\n",
        "print(f\"  2. {corpus_path} ({corpus_size_mb:.2f} MB)\")\n",
        "\n",
        "print(f\"\\nPerformance Stats:\")\n",
        "print(f\"  Total vectors indexed: {num_vectors:,}\")\n",
        "print(f\"  Embedding dimension: {dimension}\")\n",
        "print(f\"  Index memory usage: ~{index_size_mb:.1f} MB\")\n",
        "\n",
        "print(f\"\\n✓ You can now run the evaluation script!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_USiVCIhfE"
      },
      "outputs": [],
      "source": [
        "# Cell 22: Advanced Hybrid Retriever Class - Initialization\n",
        "class AdvancedHybridRetriever:\n",
        "    def __init__(self, model_path: str, index_path: str, corpus_path: str,\n",
        "                 use_bm25_plus: bool = True):\n",
        "        print(\"Initializing Advanced Hybrid Retriever...\")\n",
        "\n",
        "        self.model = SentenceTransformer(model_path)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = self.model.to(self.device)\n",
        "        print(f\" Fine-tuned model loaded on {self.device}\")\n",
        "\n",
        "        self.dense_index = faiss.read_index(index_path)\n",
        "        if torch.cuda.is_available() and hasattr(faiss, 'StandardGpuResources'):\n",
        "            try:\n",
        "                res = faiss.StandardGpuResources()\n",
        "                self.dense_index = faiss.index_cpu_to_gpu(res, 0, self.dense_index)\n",
        "                print(f\" FAISS index moved to GPU\")\n",
        "            except:\n",
        "                print(f\" FAISS index on CPU\")\n",
        "        print(f\"  Index size: {self.dense_index.ntotal:,} vectors\")\n",
        "\n",
        "        with open(corpus_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.corpus = data['corpus']\n",
        "            self.corpus_df = data.get('corpus_df', None)\n",
        "        print(f\" Corpus loaded: {len(self.corpus):,} sentences\")\n",
        "\n",
        "        print(f\"Building BM25{'Plus' if use_bm25_plus else ''} index...\")\n",
        "        tokenized_corpus = [doc.lower().split() for doc in tqdm(self.corpus, desc=\"Tokenizing\")]\n",
        "\n",
        "        if use_bm25_plus:\n",
        "            self.bm25 = BM25Plus(tokenized_corpus, k1=1.2, b=0.75, delta=1.0)\n",
        "        else:\n",
        "            self.bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "        print(\" BM25 index built\")\n",
        "\n",
        "        self.cross_encoders = []\n",
        "        cross_encoder_models = [\n",
        "            'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
        "            'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
        "        ]\n",
        "\n",
        "        for ce_model in cross_encoder_models:\n",
        "            try:\n",
        "                ce = CrossEncoder(ce_model, device=self.device, max_length=512)\n",
        "                self.cross_encoders.append(ce)\n",
        "                print(f\" Loaded cross-encoder: {ce_model}\")\n",
        "            except Exception as e:\n",
        "                print(f\" Could not load {ce_model}: {e}\")\n",
        "\n",
        "        self.use_reranking = len(self.cross_encoders) > 0\n",
        "        self.query_cache = {}\n",
        "        self.cache_enabled = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vabALDTdI1DG"
      },
      "outputs": [],
      "source": [
        "# Cell 23: Main Retrieve Method\n",
        "    def retrieve(self,\n",
        "                 query: str,\n",
        "                 top_k: int = 12,\n",
        "                 method: str = \"rrf\",\n",
        "                 dense_k: int = 100,\n",
        "                 sparse_k: int = 100,\n",
        "                 rrf_k: int = 60,\n",
        "                 rerank: bool = True,\n",
        "                 rerank_top_k: int = 50,\n",
        "                 query_expansion: bool = False,\n",
        "                 context_aware: bool = True) -> List[Tuple[str, float, int]]:\n",
        "\n",
        "        if query_expansion:\n",
        "            query = self._expand_query(query, top_n=5)\n",
        "\n",
        "        if method == \"dense\":\n",
        "            results = self._dense_retrieval(query, top_k)\n",
        "        elif method == \"bm25\":\n",
        "            results = self._bm25_retrieval(query, top_k)\n",
        "        elif method == \"hybrid\":\n",
        "            results = self._hybrid_retrieval(query, top_k, alpha=0.7)\n",
        "        elif method == \"rrf\":\n",
        "            results = self._rrf_retrieval(query, top_k, dense_k, sparse_k, rrf_k)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "        if rerank and self.use_reranking:\n",
        "            if len(results) < rerank_top_k:\n",
        "                if method == \"rrf\":\n",
        "                    candidates = self._rrf_retrieval(query, rerank_top_k, dense_k, sparse_k, rrf_k)\n",
        "                else:\n",
        "                    candidates = self._hybrid_retrieval(query, rerank_top_k, alpha=0.7)\n",
        "            else:\n",
        "                candidates = results[:rerank_top_k]\n",
        "\n",
        "            results = self._ensemble_rerank(query, candidates, top_k)\n",
        "\n",
        "        return results[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhj6ewc6I3hH"
      },
      "outputs": [],
      "source": [
        "# Cell 24: Dense Retrieval Method\n",
        "    def _dense_retrieval(self, query: str, top_k: int) -> List[Tuple[str, float, int]]:\n",
        "        cache_key = f\"dense_{query}\"\n",
        "        if self.cache_enabled and cache_key in self.query_cache:\n",
        "            all_results = self.query_cache[cache_key]\n",
        "            return all_results[:top_k]\n",
        "\n",
        "        query_emb = self.model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
        "        query_emb_np = query_emb.cpu().numpy().astype('float32').reshape(1, -1)\n",
        "        faiss.normalize_L2(query_emb_np)\n",
        "\n",
        "        scores, indices = self.dense_index.search(query_emb_np, min(top_k * 2, self.dense_index.ntotal))\n",
        "\n",
        "        results = []\n",
        "        for idx, score in zip(indices[0], scores[0]):\n",
        "            results.append((self.corpus[idx], float(score), int(idx)))\n",
        "\n",
        "        if self.cache_enabled:\n",
        "            self.query_cache[cache_key] = results\n",
        "\n",
        "        return results[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbZ6ksfzI5Zd"
      },
      "outputs": [],
      "source": [
        "# Cell 25: BM25 Retrieval Method\n",
        "    def _bm25_retrieval(self, query: str, top_k: int) -> List[Tuple[str, float, int]]:\n",
        "        tokens = query.lower().split()\n",
        "        scores = self.bm25.get_scores(tokens)\n",
        "\n",
        "        top_idx = np.argpartition(-scores, range(min(top_k, len(scores))))[:top_k]\n",
        "        top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_idx:\n",
        "            results.append((self.corpus[idx], float(scores[idx]), int(idx)))\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaQYaBEwI7An"
      },
      "outputs": [],
      "source": [
        "# Cell 26: Hybrid Retrieval Method\n",
        "    def _hybrid_retrieval(self, query: str, top_k: int, alpha: float) -> List[Tuple[str, float, int]]:\n",
        "        dense_results = self._dense_retrieval(query, top_k * 3)\n",
        "        dense_dict = {idx: score for _, score, idx in dense_results}\n",
        "\n",
        "        bm25_results = self._bm25_retrieval(query, top_k * 3)\n",
        "        bm25_dict = {idx: score for _, score, idx in bm25_results}\n",
        "\n",
        "        if dense_dict:\n",
        "            dense_scores = list(dense_dict.values())\n",
        "            dense_min, dense_max = min(dense_scores), max(dense_scores)\n",
        "            dense_dict = {k: (v - dense_min) / (dense_max - dense_min + 1e-10)\n",
        "                         for k, v in dense_dict.items()}\n",
        "\n",
        "        if bm25_dict:\n",
        "            bm25_scores = list(bm25_dict.values())\n",
        "            bm25_min, bm25_max = min(bm25_scores), max(bm25_scores)\n",
        "            bm25_dict = {k: (v - bm25_min) / (bm25_max - bm25_min + 1e-10)\n",
        "                        for k, v in bm25_dict.items()}\n",
        "\n",
        "        all_indices = set(dense_dict.keys()) | set(bm25_dict.keys())\n",
        "        combined = {}\n",
        "        for idx in all_indices:\n",
        "            d_score = dense_dict.get(idx, 0)\n",
        "            b_score = bm25_dict.get(idx, 0)\n",
        "            combined[idx] = alpha * d_score + (1 - alpha) * b_score\n",
        "\n",
        "        sorted_results = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx, score in sorted_results:\n",
        "            results.append((self.corpus[idx], float(score), int(idx)))\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPGzJU5dI9El"
      },
      "outputs": [],
      "source": [
        "# Cell 27: RRF Retrieval Method\n",
        "    def _rrf_retrieval(self, query: str, top_k: int,\n",
        "                      dense_k: int, sparse_k: int, rrf_k: int) -> List[Tuple[str, float, int]]:\n",
        "        dense_results = self._dense_retrieval(query, dense_k)\n",
        "        bm25_results = self._bm25_retrieval(query, sparse_k)\n",
        "\n",
        "        rrf_scores = defaultdict(float)\n",
        "\n",
        "        for rank, (_, _, idx) in enumerate(dense_results, 1):\n",
        "            rrf_scores[idx] += 1.0 / (rrf_k + rank)\n",
        "\n",
        "        for rank, (_, _, idx) in enumerate(bm25_results, 1):\n",
        "            rrf_scores[idx] += 1.0 / (rrf_k + rank)\n",
        "\n",
        "        sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx, score in sorted_results:\n",
        "            results.append((self.corpus[idx], float(score), int(idx)))\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxmH0gbII--d"
      },
      "outputs": [],
      "source": [
        "# Cell 28: Ensemble Reranking Method\n",
        "    def _ensemble_rerank(self, query: str, candidates: List[Tuple[str, float, int]],\n",
        "                        top_k: int) -> List[Tuple[str, float, int]]:\n",
        "        if not self.cross_encoders:\n",
        "            return candidates[:top_k]\n",
        "\n",
        "        sentences = [sent for sent, _, _ in candidates]\n",
        "        indices = [idx for _, _, idx in candidates]\n",
        "        pairs = [[query, sent] for sent in sentences]\n",
        "\n",
        "        all_scores = []\n",
        "        for ce in self.cross_encoders:\n",
        "            scores = ce.predict(pairs, show_progress_bar=False, batch_size=32)\n",
        "            all_scores.append(scores)\n",
        "\n",
        "        ensemble_scores = np.mean(all_scores, axis=0)\n",
        "\n",
        "        reranked = [(sentences[i], float(ensemble_scores[i]), indices[i])\n",
        "                   for i in range(len(sentences))]\n",
        "        reranked.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return reranked[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pYgdWO4JAty"
      },
      "outputs": [],
      "source": [
        "# Cell 29: Query Expansion Method\n",
        "    def _expand_query(self, query: str, top_n: int = 5) -> str:\n",
        "        initial_results = self._dense_retrieval(query, top_n)\n",
        "\n",
        "        word_freq = defaultdict(int)\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "                     'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been'}\n",
        "\n",
        "        for sent, _, _ in initial_results:\n",
        "            words = sent.lower().split()\n",
        "            for word in words:\n",
        "                if word not in stop_words and len(word) > 3:\n",
        "                    word_freq[word] += 1\n",
        "\n",
        "        top_terms = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "        expansion_terms = [term for term, _ in top_terms if term not in query.lower()]\n",
        "\n",
        "        if expansion_terms:\n",
        "            expanded = query + \" \" + \" \".join(expansion_terms[:2])\n",
        "            return expanded\n",
        "\n",
        "        return query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USh3fgP1JGG_"
      },
      "outputs": [],
      "source": [
        "# Cell 30: Batch Retrieve Method\n",
        "    def batch_retrieve(self, queries: List[str], **kwargs) -> List[List[Tuple[str, float, int]]]:\n",
        "        results = []\n",
        "        for query in tqdm(queries, desc=\"Batch retrieval\"):\n",
        "            results.append(self.retrieve(query, **kwargs))\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8_M9ECpJIXm"
      },
      "outputs": [],
      "source": [
        "# Cell 31: Evaluation Function\n",
        "def evaluate_retrieval_comprehensive(retriever: AdvancedHybridRetriever,\n",
        "                                    test_df: pd.DataFrame,\n",
        "                                    num_questions: int = 500,\n",
        "                                    configs: List[Dict] = None) -> Dict:\n",
        "    if configs is None:\n",
        "        configs = [\n",
        "            {\"name\": \"Dense\", \"method\": \"dense\", \"top_k\": 12, \"rerank\": False},\n",
        "            {\"name\": \"BM25\", \"method\": \"bm25\", \"top_k\": 12, \"rerank\": False},\n",
        "            {\"name\": \"Hybrid (α=0.7)\", \"method\": \"hybrid\", \"top_k\": 12, \"rerank\": False},\n",
        "            {\"name\": \"RRF\", \"method\": \"rrf\", \"top_k\": 12, \"rerank\": False},\n",
        "            {\"name\": \"RRF + Rerank\", \"method\": \"rrf\", \"top_k\": 12, \"rerank\": True, \"rerank_top_k\": 50},\n",
        "        ]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"COMPREHENSIVE EVALUATION ON {num_questions} QUESTIONS\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    test_questions = test_df.groupby('id').first().reset_index().head(num_questions)\n",
        "\n",
        "    results = {config['name']: {\n",
        "        'recall@5': [], 'recall@10': [], 'recall@12': [],\n",
        "        'precision@5': [], 'precision@10': [], 'precision@12': [],\n",
        "        'mrr': [], 'map': [], 'ndcg@10': []\n",
        "    } for config in configs}\n",
        "\n",
        "    for idx, row in tqdm(test_questions.iterrows(), total=len(test_questions), desc=\"Evaluating\"):\n",
        "        question = row['question']\n",
        "        question_id = row['id']\n",
        "\n",
        "        ground_truth = set(test_df[\n",
        "            (test_df['id'] == question_id) &\n",
        "            (test_df['is_supporting'] == 1)\n",
        "        ]['context_sentence'].tolist())\n",
        "\n",
        "        if not ground_truth:\n",
        "            continue\n",
        "\n",
        "        for config in configs:\n",
        "            try:\n",
        "                retrieved = retriever.retrieve(question, **{k: v for k, v in config.items() if k != 'name'})\n",
        "                retrieved_sentences = [sent for sent, _, _ in retrieved]\n",
        "\n",
        "                for k in [5, 10, 12]:\n",
        "                    topk_sentences = retrieved_sentences[:k]\n",
        "                    hits = sum([sent in ground_truth for sent in topk_sentences])\n",
        "\n",
        "                    recall = hits / len(ground_truth) if ground_truth else 0\n",
        "                    precision = hits / k if k > 0 else 0\n",
        "\n",
        "                    results[config['name']][f'recall@{k}'].append(recall)\n",
        "                    results[config['name']][f'precision@{k}'].append(precision)\n",
        "\n",
        "                mrr = 0\n",
        "                for rank, sent in enumerate(retrieved_sentences, 1):\n",
        "                    if sent in ground_truth:\n",
        "                        mrr = 1.0 / rank\n",
        "                        break\n",
        "                results[config['name']]['mrr'].append(mrr)\n",
        "\n",
        "                ap = 0\n",
        "                hits = 0\n",
        "                for rank, sent in enumerate(retrieved_sentences, 1):\n",
        "                    if sent in ground_truth:\n",
        "                        hits += 1\n",
        "                        ap += hits / rank\n",
        "                ap = ap / len(ground_truth) if ground_truth else 0\n",
        "                results[config['name']]['map'].append(ap)\n",
        "\n",
        "                dcg = 0\n",
        "                for rank, sent in enumerate(retrieved_sentences[:10], 1):\n",
        "                    if sent in ground_truth:\n",
        "                        dcg += 1.0 / np.log2(rank + 1)\n",
        "                idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(ground_truth), 10))])\n",
        "                ndcg = dcg / idcg if idcg > 0 else 0\n",
        "                results[config['name']]['ndcg@10'].append(ndcg)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with config {config['name']}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "\n",
        "    summary = {}\n",
        "    for config_name in results:\n",
        "        print(f\"{config_name}:\")\n",
        "        metrics_summary = {}\n",
        "        for metric in ['recall@5', 'recall@10', 'recall@12', 'precision@12', 'mrr', 'map', 'ndcg@10']:\n",
        "            if results[config_name][metric]:\n",
        "                avg = np.mean(results[config_name][metric])\n",
        "                metrics_summary[metric] = avg\n",
        "                if 'recall' in metric or 'precision' in metric or 'ndcg' in metric or 'map' in metric:\n",
        "                    print(f\"  {metric:15s}: {avg:.2%}\")\n",
        "                else:\n",
        "                    print(f\"  {metric:15s}: {avg:.4f}\")\n",
        "        print()\n",
        "        summary[config_name] = metrics_summary\n",
        "\n",
        "    return results, summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1N8wvmWJMLe"
      },
      "outputs": [],
      "source": [
        "# Cell 32: Path Configuration\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Interview/minilm-hotpotqa-epoch15-score0.5296\"\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Interview/HotpotQA_Dataset_20251128\"\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model path not found: {MODEL_PATH}\")\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    raise FileNotFoundError(f\"Dataset path not found: {DATASET_PATH}\")\n",
        "\n",
        "print(\"ADVANCED HYBRID RETRIEVAL SYSTEM\")\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Dataset: {DATASET_PATH}\")\n",
        "\n",
        "# Cell 33: Initialize Retriever\n",
        "retriever = AdvancedHybridRetriever(\n",
        "    model_path=MODEL_PATH,\n",
        "    index_path=f\"{DATASET_PATH}/faiss_index.bin\",\n",
        "    corpus_path=f\"{DATASET_PATH}/corpus.pkl\",\n",
        "    use_bm25_plus=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iklNN1cCJOkt"
      },
      "outputs": [],
      "source": [
        "# Cell 34: Load Test Data\n",
        "test_df = pd.read_parquet(f\"{DATASET_PATH}/validation_sentences.parquet\")\n",
        "print(f\" Loaded {len(test_df):,} validation sentences\")\n",
        "\n",
        "# Cell 35: Run Evaluation\n",
        "results, summary = evaluate_retrieval_comprehensive(\n",
        "    retriever=retriever,\n",
        "    test_df=test_df,\n",
        "    num_questions=500\n",
        ")\n",
        "\n",
        "# Cell 36: Save Results\n",
        "print(\"\\nSaving evaluation results...\")\n",
        "output_path = f\"{DATASET_PATH}/advanced_evaluation_results.json\"\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(f\" Results saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPdTCVr5JS0A"
      },
      "outputs": [],
      "source": [
        "# Cell 37: Best Configuration\n",
        "print(\"BEST CONFIGURATION RECOMMENDATION\")\n",
        "best_config = max(summary.items(), key=lambda x: x[1].get('recall@12', 0))\n",
        "print(f\"\\nBest method: {best_config[0]}\")\n",
        "print(f\"Recall@12: {best_config[1]['recall@12']:.2%}\")\n",
        "print(f\"MRR: {best_config[1]['mrr']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnCsV-f_JUuP"
      },
      "outputs": [],
      "source": [
        "# Cell 37: Graphs for Best Configuration and Performance Overview\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
        "\n",
        "results = {\n",
        "    \"Dense\": {\n",
        "        \"recall@5\": 50.68, \"recall@10\": 58.49, \"recall@12\": 60.90,\n",
        "        \"precision@12\": 8.29, \"mrr\": 51.15, \"map\": 40.90, \"ndcg@10\": 47.71\n",
        "    },\n",
        "    \"BM25\": {\n",
        "        \"recall@5\": 52.33, \"recall@10\": 60.27, \"recall@12\": 61.33,\n",
        "        \"precision@12\": 8.23, \"mrr\": 57.62, \"map\": 43.38, \"ndcg@10\": 51.01\n",
        "    },\n",
        "    \"Hybrid (α=0.7)\": {\n",
        "        \"recall@5\": 56.86, \"recall@10\": 67.68, \"recall@12\": 71.04,\n",
        "        \"precision@12\": 9.57, \"mrr\": 57.38, \"map\": 46.55, \"ndcg@10\": 54.31\n",
        "    },\n",
        "    \"RRF\": {\n",
        "        \"recall@5\": 61.45, \"recall@10\": 70.38, \"recall@12\": 72.36,\n",
        "        \"precision@12\": 9.79, \"mrr\": 62.90, \"map\": 50.78, \"ndcg@10\": 58.61\n",
        "    },\n",
        "    \"RRF + Rerank\": {\n",
        "        \"recall@5\": 73.10, \"recall@10\": 78.15, \"recall@12\": 78.54,\n",
        "        \"precision@12\": 10.72, \"mrr\": 78.74, \"map\": 66.90, \"ndcg@10\": 72.69\n",
        "    }\n",
        "}\n",
        "\n",
        "methods = list(results.keys())\n",
        "\n",
        "# Main comprehensive figure\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. Recall at different k\n",
        "ax1 = fig.add_subplot(gs[0, :2])\n",
        "x = np.arange(len(methods))\n",
        "width = 0.25\n",
        "\n",
        "ax1.bar(x - width, [results[m][\"recall@5\"] for m in methods], width, label='Recall@5', color=colors[0], alpha=0.8)\n",
        "ax1.bar(x,       [results[m][\"recall@10\"] for m in methods], width, label='Recall@10', color=colors[1], alpha=0.8)\n",
        "ax1.bar(x + width, [results[m][\"recall@12\"] for m in methods], width, label='Recall@12', color=colors[2], alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Method', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Recall (%)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Recall Comparison at Different k Values', fontsize=14, fontweight='bold', pad=20)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(methods, rotation=15, ha='right')\n",
        "ax1.legend(loc='upper left')\n",
        "ax1.set_ylim(0, 85)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "for bars in ax1.patches:\n",
        "    height = bars.get_height()\n",
        "    if height > 0:\n",
        "        ax1.text(bars.get_x() + bars.get_width()/2, height + 1,\n",
        "                 f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 2. Best method summary\n",
        "ax2 = fig.add_subplot(gs[0, 2])\n",
        "best = \"RRF + Rerank\"\n",
        "metrics_best = ['Recall@12', 'MRR', 'MAP', 'NDCG@10']\n",
        "values_best = [results[best][k.lower().replace('@', '@') if '@' in k else k.lower()]\n",
        "               for k in ['recall@12', 'mrr', 'map', 'ndcg@10']]\n",
        "\n",
        "bars = ax2.barh(metrics_best, values_best, color=colors[4], alpha=0.8)\n",
        "ax2.set_xlabel('Score (%)')\n",
        "ax2.set_title(f'Best Method Performance\\n({best})', fontsize=12, fontweight='bold', pad=15)\n",
        "ax2.set_xlim(0, 85)\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "for bar, val in zip(bars, values_best):\n",
        "    ax2.text(val + 1, bar.get_y() + bar.get_height()/2, f'{val:.1f}',\n",
        "             va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# 3–5. Individual metric bars\n",
        "def bar_plot(ax, values, title, ylabel, ylim):\n",
        "    bars = ax.bar(methods, values, color=colors[methods.index(ax.get_title()[-10:]) % len(colors)],\n",
        "                  alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title, fontsize=13, fontweight='bold', pad=15)\n",
        "    ax.set_xticklabels(methods, rotation=15, ha='right')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim(0, ylim)\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, h + 1, f'{h:.1f}',\n",
        "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "bar_plot(ax3, [results[m][\"mrr\"] for m in methods], \"Mean Reciprocal Rank (MRR)\", \"MRR (%)\", 85)\n",
        "\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "bar_plot(ax4, [results[m][\"map\"] for m in methods], \"Mean Average Precision (MAP)\", \"MAP (%)\", 75)\n",
        "\n",
        "ax5 = fig.add_subplot(gs[1, 2])\n",
        "bar_plot(ax5, [results[m][\"ndcg@10\"] for m in methods], \"NDCG@10\", \"NDCG@10 (%)\", 80)\n",
        "\n",
        "# 6. Heatmap\n",
        "ax6 = fig.add_subplot(gs[2, :2])\n",
        "metrics_list = ['Recall@5', 'Recall@10', 'Recall@12', 'Precision@12', 'MRR', 'MAP', 'NDCG@10']\n",
        "data = np.array([[results[m][k.lower().replace('@', '@') if '@' in k else k.lower()]\n",
        "                  for k in ['recall@5','recall@10','recall@12','precision@12','mrr','map','ndcg@10']]\n",
        "                 for m in methods])\n",
        "\n",
        "im = ax6.imshow(data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=80)\n",
        "ax6.set_xticks(np.arange(len(metrics_list)))\n",
        "ax6.set_yticks(np.arange(len(methods)))\n",
        "ax6.set_xticklabels(metrics_list, fontsize=11)\n",
        "ax6.set_yticklabels(methods, fontsize=11)\n",
        "ax6.set_title('Comprehensive Metrics Heatmap', fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "for i in range(len(methods)):\n",
        "    for j in range(len(metrics_list)):\n",
        "        ax6.text(j, i, f'{data[i,j]:.1f}', ha='center', va='center',\n",
        "                 color='black', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.colorbar(im, ax=ax6, label='Score (%)')\n",
        "\n",
        "# 7. Improvement over baseline\n",
        "ax7 = fig.add_subplot(gs[2, 2])\n",
        "baseline_recall = results['Dense']['recall@12']\n",
        "improvements = [(results[m]['recall@12'] - baseline_recall) / baseline_recall * 100\n",
        "                for m in methods[1:]]\n",
        "\n",
        "bars = ax7.barh(methods[1:], improvements, color=colors[1:], alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "ax7.set_xlabel('Improvement over Dense Baseline (%)')\n",
        "ax7.set_title('Recall@12 Improvement\\nover Dense', fontsize=12, fontweight='bold', pad=15)\n",
        "ax7.grid(axis='x', alpha=0.3)\n",
        "ax7.axvline(0, color='black', linewidth=1)\n",
        "\n",
        "for bar, val in zip(bars, improvements):\n",
        "    ax7.text(val + (1 if val > 0 else -1), bar.get_y() + bar.get_height()/2,\n",
        "             f'{val:+.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "fig.suptitle a title('HotpotQA Retrieval Performance Analysis', fontsize=18, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Interview/retrieval_performance_analysis.png',\n",
        "            dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Progressive improvement chart\n",
        "fig2, ax = plt.subplots(figsize=(12, 7))\n",
        "stages = ['Baseline\\n(Dense)', 'Add BM25\\n(Sparse)', 'Hybrid\\nFusion', 'RRF\\nFusion', 'Add Cross-\\nEncoder']\n",
        "recall_progress = [results[m]['recall@12'] for m in ['Dense', 'BM25', 'Hybrid (α=0.7)', 'RRF', 'RRF + Rerank']]\n",
        "\n",
        "ax.plot(stages, recall_progress, marker='o', linewidth=3, markersize=12, color=colors[2])\n",
        "ax.fill_between(range(len(stages)), recall_progress, alpha=0.3, color=colors[2])\n",
        "\n",
        "for i, val in enumerate(recall_progress):\n",
        "    ax.text(i, val + 1.5, f'{val:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
        "    if i > 0:\n",
        "        inc = val - recall_progress[i-1]\n",
        "        ax.annotate(f'+{inc:.1f}%', xy=(i-0.5, (val + recall_progress[i-1])/2),\n",
        "                    ha='center', fontsize=10, color='green', fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Recall@12 (%)', fontsize=14, fontweight='bold')\n",
        "ax.set_title('Progressive Improvement from Baseline to Final System', fontsize=16, fontweight='bold', pad=20)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "ax.set_ylim(50, 85)\n",
        "\n",
        "ax.axhline(65, color='red', linestyle='--', linewidth=2, alpha=0.7, label='DPR Baseline (~65%)')\n",
        "ax.axhline(80, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='ColBERT (~80%)')\n",
        "ax.axhline(85, color='green', linestyle='--', linewidth=2, alpha=0.7, label='SOTA (~85%)')\n",
        "ax.legend(loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Interview/progressive_improvement.png',\n",
        "            dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Summary table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df = pd.DataFrame(results).T.round(2)\n",
        "print(\"\\nComplete Results Table:\")\n",
        "print(df.to_string())\n",
        "\n",
        "print(\"\\nImprovements over Dense Baseline (Recall@12):\")\n",
        "baseline = results['Dense']['recall@12']\n",
        "for method in methods[1:]:\n",
        "    imp = results[method]['recall@12'] - baseline\n",
        "    rel = imp / baseline * 100\n",
        "    print(f\"{method:20s} +{imp:5.2f}%  ({rel:+5.1f}% relative)\")\n",
        "\n",
        "print(\"\\nBest Scores:\")\n",
        "for metric, key in [('Recall@12', 'recall@12'), ('MRR', 'mrr'), ('MAP', 'map'), ('NDCG@10', 'ndcg@10')]:\n",
        "    best_val = max(results[m][key] for m in methods)\n",
        "    best_method = max(methods, key=lambda m: results[m][key])\n",
        "    print(f\"  {metric:12s}: {best_val:5.2f}%  ({best_method})\")\n",
        "\n",
        "print(\"\\nAll visualizations saved to /content/drive/MyDrive/Interview/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Chart](/home/too_geeky/interview/chart.png)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Improvement](/home/too_geeky/interview/imporvement.png)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 38: Imports and Setup\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from tqdm.auto import tqdm\n",
        "import faiss\n",
        "import pickle\n",
        "from typing import List, Tuple, Dict\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "import string\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from rank_bm25 import BM25Plus\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 39: Configuration\n",
        "DATASET_INPUT_PATH = \"/kaggle/input/hotpotqa/HotpotQA_Dataset_20251128\"\n",
        "MODEL_INPUT_PATH = \"/kaggle/input/minilm/pytorch/default/1/minilm-hotpotqa-epoch15-score0.5296\"\n",
        "WORKING_PATH = \"/kaggle/working\"\n",
        "NUM_EVAL_QUESTIONS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 40: Hybrid Retriever Class\n",
        "class HybridRetriever:\n",
        "    def __init__(self, model_path: str, index_path: str, corpus_path: str):\n",
        "        \n",
        "        self.model = SentenceTransformer(model_path)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = self.model.to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            self.model.half()\n",
        "        self.model.eval()\n",
        "        \n",
        "        self.dense_index = faiss.read_index(index_path)\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                res = faiss.StandardGpuResources()\n",
        "                self.dense_index = faiss.index_cpu_to_gpu(res, 0, self.dense_index)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        with open(corpus_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.corpus = data['corpus']\n",
        "        \n",
        "        tokenized_corpus = [doc.lower().split() for doc in tqdm(self.corpus, desc=\"Building BM25\", leave=False)]\n",
        "        self.bm25 = BM25Plus(tokenized_corpus)\n",
        "        \n",
        "        try:\n",
        "            self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', \n",
        "                                             device=self.device, max_length=512)\n",
        "        except:\n",
        "            self.cross_encoder = None\n",
        "        \n",
        "        print(f\" Retriever ready on {self.device}\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 2, min_score: float = 0.5) -> List[Tuple[str, float, int]]:\n",
        "        candidates = self._rrf_retrieval(query, top_k * 5, dense_k=100, sparse_k=100, rrf_k=60)\n",
        "        \n",
        "        if self.cross_encoder and len(candidates) > 0:\n",
        "            candidates = self._rerank(query, candidates[:50], top_k * 3)\n",
        "        \n",
        "        filtered = [(sent, score, idx) for sent, score, idx in candidates if score > min_score]\n",
        "        \n",
        "        if filtered:\n",
        "            return filtered[:top_k]\n",
        "        else:\n",
        "            return candidates[:1]\n",
        "\n",
        "    def _dense_retrieval(self, query: str, top_k: int) -> List[Tuple[str, float, int]]:\n",
        "        with torch.no_grad():\n",
        "            query_emb = self.model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
        "            if self.device == 'cuda':\n",
        "                query_emb = query_emb.cpu()\n",
        "            query_emb_np = query_emb.numpy().astype('float32').reshape(1, -1)\n",
        "        faiss.normalize_L2(query_emb_np)\n",
        "        scores, indices = self.dense_index.search(query_emb_np, min(top_k, self.dense_index.ntotal))\n",
        "        return [(self.corpus[idx], float(score), int(idx)) for idx, score in zip(indices[0], scores[0])]\n",
        "\n",
        "    def _bm25_retrieval(self, query: str, top_k: int) -> List[Tuple[str, float, int]]:\n",
        "        tokens = query.lower().split()\n",
        "        scores = self.bm25.get_scores(tokens)\n",
        "        top_idx = np.argpartition(-scores, range(min(top_k, len(scores))))[:top_k]\n",
        "        top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
        "        return [(self.corpus[idx], float(scores[idx]), int(idx)) for idx in top_idx]\n",
        "\n",
        "    def _rrf_retrieval(self, query: str, top_k: int, dense_k: int, \n",
        "                      sparse_k: int, rrf_k: int) -> List[Tuple[str, float, int]]:\n",
        "        dense_results = self._dense_retrieval(query, dense_k)\n",
        "        bm25_results = self._bm25_retrieval(query, sparse_k)\n",
        "        \n",
        "        rrf_scores = defaultdict(float)\n",
        "        for rank, (_, _, idx) in enumerate(dense_results, 1):\n",
        "            rrf_scores[idx] += 1.0 / (rrf_k + rank)\n",
        "        for rank, (_, _, idx) in enumerate(bm25_results, 1):\n",
        "            rrf_scores[idx] += 1.0 / (rrf_k + rank)\n",
        "        \n",
        "        sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        return [(self.corpus[idx], float(score), int(idx)) for idx, score in sorted_results]\n",
        "\n",
        "    def _rerank(self, query: str, candidates: List[Tuple[str, float, int]], \n",
        "               top_k: int) -> List[Tuple[str, float, int]]:\n",
        "        if not self.cross_encoder:\n",
        "            return candidates[:top_k]\n",
        "        \n",
        "        sentences = [sent for sent, _, _ in candidates]\n",
        "        indices = [idx for _, _, idx in candidates]\n",
        "        pairs = [[query, sent] for sent in sentences]\n",
        "        \n",
        "        scores = self.cross_encoder.predict(pairs, show_progress_bar=False, batch_size=32)\n",
        "        reranked = [(sentences[i], float(scores[i]), indices[i]) for i in range(len(sentences))]\n",
        "        reranked.sort(key=lambda x: x[1], reverse=True)\n",
        "        return reranked[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 41: QA System Class\n",
        "class QwenQASystem:\n",
        "    def __init__(self, model_name: str = \"Qwen/Qwen2.5-7B-Instruct\", \n",
        "                 lora_path: str = None, load_in_8bit: bool = True):\n",
        "        \n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        if load_in_8bit:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=6.0,\n",
        "                llm_int8_enable_fp32_cpu_offload=True\n",
        "            )\n",
        "        else:\n",
        "            quantization_config = None\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name, trust_remote_code=True, padding_side='left'\n",
        "        )\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if not load_in_8bit else None,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        \n",
        "        if lora_path:\n",
        "            from peft import PeftModel\n",
        "            self.model = PeftModel.from_pretrained(self.model, lora_path)\n",
        "            self.model = self.model.merge_and_unload()\n",
        "        \n",
        "        self.model.eval()\n",
        "        print(f\"✓ Model loaded on {self.device}\")\n",
        "\n",
        "    def generate_answer(self, question: str, context: str = None) -> str:\n",
        "        if context:\n",
        "            prompt = f\"\"\"Extract ONLY the answer from the context. Give the shortest possible answer - just the name, number, or phrase needed. Do not explain.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (2-5 words max):\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"Question: {question}\n",
        "\n",
        "Answer (2-5 words max):\"\"\"\n",
        "        \n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=30,\n",
        "                temperature=1.0,\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        if \"Answer:\" in full_response:\n",
        "            answer = full_response.split(\"Answer:\")[-1].strip()\n",
        "        else:\n",
        "            answer = full_response[len(prompt):].strip()\n",
        "        \n",
        "        answer = answer.split('\\n')[0].strip()\n",
        "        \n",
        "        return answer\n",
        "\n",
        "    def answer_with_strategy(self, question: str, context: str, retrieval_score: float,\n",
        "                             min_confidence: float = 0.6) -> Tuple[str, str]:\n",
        "        answer_no_rag = self.generate_answer(question, context=None)\n",
        "        \n",
        "        if retrieval_score < min_confidence:\n",
        "            return answer_no_rag, \"no_rag_low_confidence\"\n",
        "        \n",
        "        answer_with_rag = self.generate_answer(question, context=context)\n",
        "        \n",
        "        if len(answer_with_rag) > len(answer_no_rag) * 2.5:\n",
        "            return answer_no_rag, \"no_rag_verbose\"\n",
        "        \n",
        "        low_confidence_phrases = [\"no answer\", \"not mentioned\", \"not provided\", \n",
        "                                 \"cannot answer\", \"no information\"]\n",
        "        if any(phrase in answer_with_rag.lower() for phrase in low_confidence_phrases):\n",
        "            return answer_no_rag, \"no_rag_uncertain\"\n",
        "        \n",
        "        return answer_with_rag, \"rag\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 42: Evaluation Metrics\n",
        "def normalize_answer(s: str) -> str:\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction: str, ground_truth: str) -> float:\n",
        "    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def compute_f1(prediction: str, ground_truth: str) -> float:\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    truth_tokens = normalize_answer(ground_truth).split()\n",
        "    \n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    \n",
        "    common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    \n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    \n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(truth_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 43: Evaluation Function\n",
        "def evaluate_system(retriever, qa_system, test_df, num_questions=100):\n",
        "    print(f\"\\nEvaluating {num_questions} questions...\")\n",
        "    print(\"Fixes applied:\")\n",
        "    print(\"   Reduced context: top_k = 2\")\n",
        "    print(\"   Confidence filtering: min_score = 0.5\")\n",
        "    print(\"   Fixed prompts: max_tokens = 30, temp = 0.0\")\n",
        "    print(\"   Hybrid decision strategy\\n\")\n",
        "    \n",
        "    unique_questions = test_df.groupby('id').first().reset_index().head(num_questions)\n",
        "    \n",
        "    results = {\n",
        "        'original_no_rag': {'predictions': [], 'ground_truths': []},\n",
        "        'fixed_with_rag': {'predictions': [], 'ground_truths': []},\n",
        "        'smart_hybrid': {'predictions': [], 'ground_truths': []}\n",
        "    }\n",
        "    \n",
        "    question_details = []\n",
        "    strategy_counts = defaultdict(int)\n",
        "    \n",
        "    for idx, row in tqdm(unique_questions.iterrows(), total=len(unique_questions)):\n",
        "        question = row['question']\n",
        "        ground_truth = row['answer']\n",
        "        \n",
        "        try:\n",
        "            answer_no_rag = qa_system.generate_answer(question, context=None)\n",
        "            em_no_rag = compute_exact_match(answer_no_rag, ground_truth)\n",
        "            f1_no_rag = compute_f1(answer_no_rag, ground_truth)\n",
        "        except:\n",
        "            answer_no_rag = \"\"\n",
        "            em_no_rag = f1_no_rag = 0\n",
        "        \n",
        "        results['original_no_rag']['predictions'].append(answer_no_rag)\n",
        "        results['original_no_rag']['ground_truths'].append(ground_truth)\n",
        "        \n",
        "        retrieved = retriever.retrieve(question, top_k=2, min_score=0.5)\n",
        "        \n",
        "        if not retrieved:\n",
        "            context = \"\"\n",
        "            avg_score = 0.0\n",
        "        else:\n",
        "            context = \"\\n\\n\".join([f\"[{i+1}] {sent}\" for i, (sent, _, _) in enumerate(retrieved)])\n",
        "            avg_score = np.mean([score for _, score, _ in retrieved])\n",
        "        \n",
        "        try:\n",
        "            answer_rag = qa_system.generate_answer(question, context=context)\n",
        "            em_rag = compute_exact_match(answer_rag, ground_truth)\n",
        "            f1_rag = compute_f1(answer_rag, ground_truth)\n",
        "        except:\n",
        "            answer_rag = \"\"\n",
        "            em_rag = f1_rag = 0\n",
        "        \n",
        "        results['fixed_with_rag']['predictions'].append(answer_rag)\n",
        "        results['fixed_with_rag']['ground_truths'].append(ground_truth)\n",
        "        \n",
        "        try:\n",
        "            answer_hybrid, strategy = qa_system.answer_with_strategy(question, context, avg_score)\n",
        "            strategy_counts[strategy] += 1\n",
        "            em_hybrid = compute_exact_match(answer_hybrid, ground_truth)\n",
        "            f1_hybrid = compute_f1(answer_hybrid, ground_truth)\n",
        "        except:\n",
        "            answer_hybrid = \"\"\n",
        "            strategy = \"error\"\n",
        "            em_hybrid = f1_hybrid = 0\n",
        "        \n",
        "        results['smart_hybrid']['predictions'].append(answer_hybrid)\n",
        "        results['smart_hybrid']['ground_truths'].append(ground_truth)\n",
        "        \n",
        "        question_details.append({\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'answer_no_rag': answer_no_rag,\n",
        "            'answer_fixed_rag': answer_rag,\n",
        "            'answer_smart': answer_hybrid,\n",
        "            'strategy': strategy,\n",
        "            'retrieval_score': avg_score,\n",
        "            'num_contexts': len(retrieved),\n",
        "            'em_no_rag': em_no_rag,\n",
        "            'f1_no_rag': f1_no_rag,\n",
        "            'em_fixed_rag': em_rag,\n",
        "            'f1_fixed_rag': f1_rag,\n",
        "            'em_smart': em_hybrid,\n",
        "            'f1_smart': f1_hybrid\n",
        "        })\n",
        "    \n",
        "    metrics = {}\n",
        "    for method_name in ['original_no_rag', 'fixed_with_rag', 'smart_hybrid']:\n",
        "        preds = results[method_name]['predictions']\n",
        "        truths = results[method_name]['ground_truths']\n",
        "        \n",
        "        em_scores = [compute_exact_match(p, t) for p, t in zip(preds, truths)]\n",
        "        f1_scores = [compute_f1(p, t) for p, t in zip(preds, truths)]\n",
        "        \n",
        "        metrics[method_name] = {\n",
        "            'em': np.mean(em_scores),\n",
        "            'f1': np.mean(f1_scores),\n",
        "            'avg_length': np.mean([len(p) for p in preds])\n",
        "        }\n",
        "    \n",
        "    print(\"\\nRESULTS COMPARISON\")\n",
        "\n",
        "    print(f\"{'Method':<25} {'EM':>10} {'F1':>10} {'Avg Length':>12}\")\n",
        "\n",
        "    \n",
        "    for method_name, method_label in [\n",
        "        ('original_no_rag', 'Original (No RAG)'),\n",
        "        ('fixed_with_rag', 'Fixed (Always RAG)'),\n",
        "        ('smart_hybrid', 'Smart Hybrid')\n",
        "    ]:\n",
        "        m = metrics[method_name]\n",
        "        print(f\"{method_label:<25} {m['em']:>9.1%} {m['f1']:>9.1%} {m['avg_length']:>11.1f}\")\n",
        "    \n",
        "    print(\"\\nIMPROVEMENTS vs ORIGINAL NO-RAG\")\n",
        "    \n",
        "    baseline_em = metrics['original_no_rag']['em']\n",
        "    baseline_f1 = metrics['original_no_rag']['f1']\n",
        "    \n",
        "    for method_name, method_label in [('fixed_with_rag', 'Fixed RAG'), ('smart_hybrid', 'Smart Hybrid')]:\n",
        "        em_improvement = (metrics[method_name]['em'] - baseline_em) / (baseline_em + 1e-10) * 100\n",
        "        f1_improvement = (metrics[method_name]['f1'] - baseline_f1) / (baseline_f1 + 1e-10) * 100\n",
        "        \n",
        "        print(f\"\\n{method_label}:\")\n",
        "        print(f\"  EM: {em_improvement:+.1f}%\")\n",
        "        print(f\"  F1: {f1_improvement:+.1f}%\")\n",
        "    \n",
        "    print(\"\\nSMART HYBRID STRATEGY BREAKDOWN\")\n",
        "    for strategy, count in sorted(strategy_counts.items(), key=lambda x: -x[1]):\n",
        "        print(f\"  {strategy}: {count} ({count/len(question_details)*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'metrics': metrics,\n",
        "        'question_details': question_details,\n",
        "        'strategy_counts': dict(strategy_counts)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 44: Load Dataset\n",
        "print(\"Loading dataset...\")\n",
        "val_df = pd.read_parquet(f\"{DATASET_INPUT_PATH}/validation_sentences.parquet\")\n",
        "print(f\" Loaded: {len(val_df):,} rows, {val_df['id'].nunique():,} questions\")\n",
        "\n",
        "# Cell 45: Initialize Retriever\n",
        "print(\"Initializing retriever...\")\n",
        "retriever = HybridRetriever(\n",
        "    model_path=MODEL_INPUT_PATH,\n",
        "    index_path=f\"{WORKING_PATH}/faiss_index.bin\",\n",
        "    corpus_path=f\"{WORKING_PATH}/corpus.pkl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 46: Initialize QA System\n",
        "print(\"Initializing QA system...\")\n",
        "qa_system = QwenQASystem(\n",
        "    model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    lora_path=None,\n",
        "    load_in_8bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 47: Run Evaluation\n",
        "print(\"Running evaluation...\")\n",
        "results = evaluate_system(\n",
        "    retriever=retriever,\n",
        "    qa_system=qa_system,\n",
        "    test_df=val_df,\n",
        "    num_questions=NUM_EVAL_QUESTIONS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 48: Save Results\n",
        "output_path = f\"{WORKING_PATH}/fixed_rag_results.json\"\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump({\n",
        "        'metrics': results['metrics'],\n",
        "        'strategy_counts': results['strategy_counts']\n",
        "    }, f, indent=2)\n",
        "print(f\"\\n Results saved: {output_path}\")\n",
        "\n",
        "detailed_df = pd.DataFrame(results['question_details'])\n",
        "detailed_csv = f\"{WORKING_PATH}/fixed_detailed_results.csv\"\n",
        "detailed_df.to_csv(detailed_csv, index=False)\n",
        "print(f\" Detailed results: {detailed_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![RAG-Performance](/home/too_geeky/interview/rag_analysis_plots.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.10.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
